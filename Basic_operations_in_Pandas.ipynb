{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# How To Use Conda Environment In a Jupyter Notebook\n",
        "conda create -n MyEnv python=3.13.0 # lastest 3.13.0/1/2/3 # 1. Create 'MyEnv' named Environment with given Python Version (Go through Anaconda Navigator to see avaialble Python versions)\n",
        "conda activate MyEnv              # 2. Activating a Conda Environment\n",
        "pip install ipykernel             # 3. IPython kernel is installed in your environment\n",
        "python -m ipykernel install --user --name=KernalName  # 4. Create a kernel for the conda environment that can be used in Jupyter Notebook (so that new env will have new Kernal)\n",
        "pip install notebook              # 5. install jupyter notebook is not found in the environment\n",
        "pip install nbclassic               # this opens the regular notebook (else default notebook is opened for the selected environment)\n",
        "conda install numpy               # 6. Installing numpy Packages in a Conda Environment\n",
        "conda install -c anaconda scikit-learn  # 7. Install sklearn if necessary |or| pip install scikit-learn\n",
        "(my_env)D:\\myFolder>jupyter nbclassic   # 8. Opens the regular notebook\n",
        "(my_env)D:\\myFolder>conda deactivate   # 9. my_env environment is deactivated\n",
        "\n",
        "conda list python  # select which version of python is installed on the current evn in the conda prompt\n",
        "conda env list  # list of environments made\n",
        "conda env remove --name myenv # remove the environment\n",
        "jupyter kernelspec list  # list of installed kernals on conda\n",
        "jupyter kernelspec remove KernalName  # remove the kernal from conda\n",
        "\n",
        "\n",
        "# list of environments made in jupyter notebook\n",
        "import subprocess\n",
        "def list_conda_envs():\n",
        "    result = subprocess.run(['conda', 'env', 'list'], stdout=subprocess.PIPE, text=True)\n",
        "    print(result.stdout)\n",
        "list_conda_envs()\n",
        "\n",
        "#======================================================================================\n",
        "import os\n",
        "import sys\n",
        "os.getcwd()  # import os  # cross check in which folder the jupyter notebook is running\n",
        "sys.executable   # import sys   # cross check in which environmne the code is running\n",
        "\n",
        "!conda activate <environment_name>   # activate environment using jupyter notebook\n",
        "\n",
        "import json    # check which kernal in jupyter notebook which is getting used by the code\n",
        "with open(\"Untitled1.ipynb\", \"r\", encoding=\"utf-8\") as notebook_file:  # Get the notebook metadata\n",
        "    notebook_content = json.load(notebook_file)\n",
        "notebook_content[\"metadata\"][\"kernelspec\"][\"name\"]  # Extract the kernel name from the metadata\n",
        "jupyter kernelspec remove KernalName  # removes the kernal with KernalName\n",
        "\n",
        "import sys   # check python version\n",
        "print(sys.version)\n"
      ],
      "metadata": {
        "id": "f3yGwqWDKGBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlSC93OwyRXF"
      },
      "outputs": [],
      "source": [
        "Ref link: https://pandas.pydata.org/pandas-docs/version/0.17.0/api.html\n",
        "\n",
        "jupyter notebook --notebook-dir=D:   # opens the jpyter notebook from specified drive location\n",
        "\n",
        "pd.set_option('display.max_columns', None) # show all columns in df\n",
        "pd.set_option(\"display.max_colwidth\", None)  # or # pd.set_option('max_colwidth',None)\n",
        "pd.reset_option('max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)   # show all rows of the df/result output\n",
        "pd.set_option(‘precision’, 2)            # Setting precision to 2 decimal places\n",
        "np.set_printoptions(threshold=sys.maxsize)   # import sys  (to see all of the elements in the array)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#======================= OS library related functions =============================\n",
        "import os\n",
        "os.getcwd()  #  Get the current working directory (CWD)\n",
        "os.chdir(\"path\") #Change Directory to Drive\n",
        "os.mkdir('path') # Create the directory\n",
        "os.makedirs('path') # Create a directory recursively. (while making leaf directory if any intermediate-level directory is missing, os.makedirs() method will create them all.)\n",
        "dir_list = os.listdir('path') # Get the list of all files and directories in the root directory\n",
        "os.remove('path+file.txt') # Remove/delete the file specified ( If the specified path is a directory then OSError will be raised by the method.)\n",
        "os.rmdir('path')  # Remove/delete an empty directory. (OSError will be raised if the specified path is not an empty directory.)\n",
        "\n",
        "#======================= Get current date and time  ================================\n",
        "from datetime import datetime\n",
        "Date_Time = datetime.now()\n",
        "Day = pd.Timestamp(Checking_Date).day_name()\n",
        "\n",
        "#========================= time differece, cell execution time ==========================\n",
        "import time\n",
        "start = time.time()\n",
        "...\n",
        "...\n",
        "...\n",
        "end = time.time()\n",
        "estimated_time = round((end-start)*1000, 4)\n",
        "print(estimated_time, 'ms')\n",
        "\n",
        "#======================= Get names and size of variables ================================\n",
        "# -------- names of variables running in the programs ---------------\n",
        "print(dir())       # prints out the list of all the variables and methods in the given program\n",
        "\n",
        "#--------------- get the size of an object in bytes --------------\n",
        "import sys\n",
        "str(sys.getsizeof('VariableName')) + \" Bytes\"  # or use following line of code\n",
        "print(''.__sizeof__())\n",
        "\n",
        "#----------- get size of variable in MB -----------------\n",
        "%whos # returns all variables, dataframes, functions, figures, etc used in jupyter notebook\n",
        "str((VariableName.memory_usage().sum()/1024**2).round(2)) +\" Mb\"   # returns memory occupied by variable im MB\n",
        "\n",
        "#========================= use variable of one jupyter notebook in another notebook ==============\n",
        "%store variable_name     # store the variable in jupyter notebook 1\n",
        "%store -r variable_name  # retrive the variable in another jupyter notebook 2\n",
        "%store -z variable_name  # deletes the stored variable from jupyter notebook 1\n",
        "# if above doesn't work then follow the following\n",
        "#--------------------------------------\n",
        "# For Dataframe\n",
        "df.to_pickle('df.pkl')  # Save DataFrame to a pickle file in notebook 1\n",
        "df = pd.read_pickle('df.pkl')  # Load DataFrame from the pickle file in notebook 2\n",
        "\n",
        "#--------------------------------------\n",
        "# For other variables (lists, dictionaries)\n",
        "import pickle\n",
        "with open('variable.pkl', 'wb') as f:   # save variable in notebook 1\n",
        "    pickle.dump(variable, f)\n",
        "\n",
        "import pickle\n",
        "with open('variable.pkl', 'rb') as f:   # load variable in notebook 2\n",
        "    variable = pickle.load(f)\n",
        "\n",
        "# ************************* CLEANING THE DATASET *************************\n",
        "# ---- renaming/cleaning the column names ----\n",
        "df.columns=['NewName1','NewName2','NewNameN3']            # Renaming the column (No of old column Names = No of New column Names to be given)\n",
        "df.rename(index={1:'first'}, columns={'OldName':'NewName'}, inplace = True)   # Renaming the index and column\n",
        "df.columns = df.columns.str.replace(\" \",\"_\")              # remove blank space between two words in column name\n",
        "\n",
        "# ---- methodes to handle text data in the column -------\n",
        "str.lower()/upper()/title()/replace('old','new')/lstrip()/rstrip()/strip()/len()/split()\n",
        "str.contains()/endswith()/startswith()\n",
        "# replace(), contains(), split() can handle regex\n",
        "\n",
        "#--- shortinig the too long string ------\n",
        "import textwrap\n",
        "textwrap.shorten(\"any long string\", width=12, placeholder=\"...\")  # output-> 'any long...'\n",
        "\n",
        "# ---- drop column and row -----\n",
        "df.drop(index=1, axis=0, inplace=True)   # delete row of index no 1, and do immediately : df.reset_index(drop=True, inplace=True)\n",
        "df.drop(['Col1','Col2','Col3'], axis=1, inplace=True)\n",
        "df.drop(df.iloc[:,2:],axis=1,inplace=True)   # drop columns from col no 2 onwards (col number starts from 0)\n",
        "\n",
        "# ---- add row / insert row in the (specified) end of the location ----\n",
        "df.loc[len(df.index)] = [int/float/str/NaN values as per columns] # adds row and specified contents at the end of the df\n",
        "\n",
        "# ---- copy the contents of one column to another column, (on the basis of condition) if NaN then skip the copying (within the same dataframe)\n",
        "df.loc[df['col1'].notnull(), 'col2'] = df['col1']  # copy the contents from 'col1' to 'col2' where 'col1' is_not_null (have some value)\n",
        "\n",
        "# === lambda v/s np.where v/s mask (modifying column contents with condition) - 1 ===\n",
        "df[\"col\"] = df[\"col\"].apply(lambda x : x+1111 if x>500000 else x)       # if (col contents > 5,00,000) then add 1111 in col contents else dont add /\n",
        "df[\"col\"] = np.where(df[\"col\"] >500000, df[\"col\"]+1111, df[\"col\"])      # if (col contents > 5,00,000) then add 1111 in col contents else dont add / more efficient for large datasets as it performs the operation in a vectorized manner.\n",
        "df['temp'].where(df['temp']>24, 30)    # if temp>24, assign 30, else keep value as it is. (if 30 is not specified then assign NaN)\n",
        "df['temp'].mask(df['temp']>24, 10)     # if temp>24, mask/replace values by 10, else keep values as it is. (if 10 is not specified then assign NaN)\n",
        "# df[df['Age'].isin([40,43,56,32])]    # use for multiple entries\n",
        "\n",
        "'''where: Faster, Most Mem Efficient, less flexible, Ideal when retaining most original values and replacing a subset\n",
        "mask: Slow, Slightly less Mem Efficient, less flexible, Ideal when replacing a subset of values with a common value\n",
        "lambda: Slowest, Mem Intensive, more flexible as it applies custom logic to each element, Useful for complex conditions or when specific element-wise operations are required'''\n",
        "\n",
        "# multiple conditions in lambda\n",
        "df['Sex_New']=df['Sex'].apply(lambda x : 'male' if x.startswith('m') else 'female' if x.startswith('f') else x)\n",
        "largest_num = lambda a,b,c : a if a>b and a>c else b if b>a and b>c else c if c>a and c>b else a\n",
        "# -------------- split col categories (Name+Surname) and put it in seprate column -----------------\n",
        "df.insert(loc=1, column='First_Name', value=df['Full_Name'].str.split(n=1,expand=True).get(0)) # splitted col 'First_Name' is placed at index col no 1\n",
        "# n=1 : whole string is splitted in two (single split). Ex: ['first_name','middle_name last_name'], default: all split\n",
        "# expand=True : returns in the form of DataFrame else Series containig list of string\n",
        "# get(0) : first value (Name) will be returned, get(1) : second value (Surname) will be returned\n",
        "\n",
        "# The employee having name starts with 'M' will get 0 bonus and rest of the employees get a 100% bonus.\n",
        "employees['bonus'] = employees.apply(lambda x : 0 if x['name'].startswith('M') else x['salary'], axis=1)\n",
        "\n",
        "#====------- string operations using regex --------==============\n",
        "# Delete anything within brackets or anything\n",
        "#Ex: \"I am from (Country) India\" -output-> \"I am from India\"\n",
        "df['Cat_col'] = df['Cat_col'].apply(lambda x : re.sub(\"\\(.*?\\)\",\"\",x) if (x.rfind(\"(\")>0) else x) # deleting unwanted '()'and contents\n",
        "\n",
        "# delete everything before specified '--' from the given string\n",
        "str1 = 'Anand Pvt Limited -- 100000_00.xls'\n",
        "re.sub(r'^.*?--', '', str1).strip()   # --> '100000_00.xls'\n",
        "#(or)\n",
        "str1[str1.index('100'):]   # output -->'100000_00.xls'\n",
        "\n",
        "# delete everything after specified character in a string\n",
        "str1 = 'Mydata_23_00500504.xls'\n",
        "str1[ str1.index(\"_005\") : ]      # output --> '_00500504.xls'\n",
        "\n",
        "\n",
        "#=====------- comparing the categories in % within same category_column 1 -----======\n",
        "## this progg returns matching categories and match values in % if >80 match is found in categories within the same categorical column\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "def fuzzy_fun(x):\n",
        "    for i in x:\n",
        "        matched_score = process.extract(i, x, scorer=fuzz.token_sort_ratio)\n",
        "        cnt=0\n",
        "        for j in matched_score:\n",
        "            if (j[1] > 80 and j[1]<100):\n",
        "                if cnt>0:     # firstly a catergory cmpares with itself and 100% result is obtained. The \"if\" condition avoids this.\n",
        "                    print(\"\\n\",i, \"<-->\",j[0], \"| match\",j[1],\"%\")\n",
        "\n",
        "                    uncomm_words_str1 = []             # this block returns the unmatched words from str1 and str2\n",
        "                    uncomm_words_str2 = []\n",
        "                    for ii in i.split():\n",
        "                        if ii not in j[0].split():\n",
        "                            uncomm_words_str1.append(ii)\n",
        "                    for ii in j[0].split():\n",
        "                        if ii not in i.split():\n",
        "                            uncomm_words_str2.append(ii)\n",
        "                    print('\\nUnmatched word from str1=',uncomm_words_str1)\n",
        "                    print('Unmatched word from str2=',uncomm_words_str2)    # unmatched words block ends here (this block can be deleted if not required)\n",
        "                cnt=cnt+1\n",
        "        print(\"*\"*50)\n",
        "    print(\"Done\")\n",
        "\n",
        "fuzzy_fun(df['ColName'].unique())\n",
        "\n",
        "# Ref: https://github.com/thuynh323/Natural-language-processing/blob/master/FuzzyWuzzy%20-%20Ramen%20Rater%20List/Find%20similar%20strings%20with%20FuzzyWuzzy.ipynb\n",
        "\n",
        "##############################################################################################################################\n",
        "#=====------- comparing the categories in % within same category_column 2 -----======\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "text1 = \"Medical Records Summary (Page T of 2 )\\n\\noartara0zt Pa 1-3, inal\\nConsult eter\\noan\\n\\n1D\"\n",
        "text2 = \"Medical (Page T of 2 )\\n\\noartara0zt Pa 1-3, inal\\nConsult eter”\\noan\\n\\n1D\"\n",
        "\n",
        "similarity = SequenceMatcher(a=text1, b=text2, autojunk=True)\n",
        "round(similarity.ratio(), 2)\n",
        "#----------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# === replace (modifying column contents) - 2 ===\n",
        "dict_replace = {'M':'Men', 33.32:33, disease:Diseasaes}       # categories/contents to be replaced from different columns of a dataframe\n",
        "df.replace(dict_replace, inplace=True)        # replaces the occurances of category/contents of different columns of df with said replacement list in given list\n",
        "df.replace(np.nan, 0, inplace=True)           # replaces the all occurances of NaN by 0\n",
        "\n",
        "# NaN values can also be replaced with said values using replace()\n",
        "\n",
        "# === map (modifying column contents : adding new column with new contents with reference to old columns) - 3 ===\n",
        "df = pd.DataFrame( {'particulars':['ball','pencil','pen','mug','ashtray','pencil'],\n",
        "                    'price': [12.33,11.44,33.21,13.23,33.62, 22.33]})\n",
        "new_price = {'ball': 12, 'pencil':11, 'pen':'Not Avl','mug':13}\n",
        "df['new_price'] = df['particulars'].map(new_price)            # creates a new col 'new_price' and maps given new price to 'particular' column\n",
        "\n",
        "# ==== try, except, else, finally ====\n",
        "try:              # try this block\n",
        "    c = ((a+b) // (a-b))\n",
        "except:           # if error occurs, then execute this block\n",
        "    print(\"Oops!\", sys.exc_info()[0], \"occurred.\")     # import sys (this line returns exact error name)\n",
        "else:             # if no error, then execute this block\n",
        "    print(c)\n",
        "finally:          # this block is always executed regardless of exception generation.\n",
        "    print('This is always executed')\n",
        "\n",
        "# ------------example-------------\n",
        "# === .apply()\n",
        "import sys\n",
        "\n",
        "def conversion(x):\n",
        "    try:                              # try-except is used to handle any NaN values (else error is displayed)\n",
        "        if x.endswith(\"Kokate\"):      # if any full name ends with 'Kokate'...\n",
        "            x = x.replace(\"Kokate\", \"\")  #...replace that 'Kokate' with blank...\n",
        "            return x                     #...return store/return modified string back to column\n",
        "        else:                         # if any full name ends with 'Kokate' then return store/return modified string back to column\n",
        "            print(\"x in else body:\",x)\n",
        "            return x\n",
        "    except:\n",
        "        print(\"Oops!\", sys.exc_info()[0], \"occurred.\")  # handle the error\n",
        "\n",
        "df['Candidate_Full_Name'] = df['Candidate_Full_Name'].apply(conversion)    # if 'Candidate_Full_Name' column contains any string (named \"Kokate\") to be removed then we apply .apply()\n",
        "\n",
        "# === raise error =====\n",
        "x = -1\n",
        "if x < 0:\n",
        "  raise Exception(\"Sorry, no numbers below zero\")\n",
        "# ref : https://www.programiz.com/python-programming/exception-handling\n",
        "\n",
        "# =================== EDA OF THE DATASET ====================\n",
        "df.info()\n",
        "df.describe() # include=['all','object','int','float''number']|.round()\n",
        "df.shape\n",
        "df.columns  # returns list of columns\n",
        "df.dtypes   # returns data type of each columns in a df\n",
        "df.index    # returns index range\n",
        "df.values   # returns table values (except index)\n",
        "\n",
        "# ------- value_counts in percentage: get value count in the form of dataframe with columns as 'Category_Name', 'Count', '%Count' --------------\n",
        "df['ColName'].value_counts(normalize = True)*100           # returns the occurance of the category in percentage\n",
        "# or\n",
        "df1 = pd.DataFrame(df['ColName'].value_counts()).rename(columns={'ColName':'Count'})\n",
        "df2 = pd.DataFrame((df['ColName'].value_counts()*100/df['ColName'].value_counts().sum()).round(2)).rename(columns={'ColName':'%Count'})\n",
        "df3 = pd.concat(objs=[df1,df2], axis=1)\n",
        "df3.reset_index(inplace=True)\n",
        "df3.rename(columns={'index':'ColName'},inplace=True)\n",
        "df3\n",
        "#-------------------------------\n",
        "\n",
        "df.select_dtypes(include='float/number/object', exclude='datetime64')          # returns a dataset with specified float/int/object columns type\n",
        "df.select_dtypes(include='float/number/object', exclude='datetime64').columns  # returns column names with specified float/int/object columns type\n",
        "\n",
        "df['Sex'].nunique()               # returns unique category count, ex: 2\n",
        "df['Sex'].nunique(dropna=False)   # returns unique category count considering NaN also, ex: 3\n",
        "\n",
        "#===== NULL values / missing vlaues =====\n",
        "df.count()                                             # returns total count of (non-null) filled cells of the columns\n",
        "df.isnull().sum()                                      # total count of Null cells in the specified dataframe\n",
        "# null values count and % in descending order\n",
        "pd.concat(objs=[df.isnull().sum(), round(df.isnull().sum()/len(df) * 100, 2)], axis=1).rename(columns={0:'NullCnt', 1:'NullCnt%'}).sort_values(by=['NullCnt'], ascending=False)\n",
        "df.isnull().sum()[df.isnull().sum() != 0]              # viewing only those columns which has other than 0 null values\n",
        "df[pd.isnull(df['Gender'])]                            # returns only those rows of whole dataset where 'Gender' is missing\n",
        "((df.isnull().sum()[df.isnull().sum() != 0])*100/len(df)).round(2)  # view null value columns only with % null count\n",
        "df.dropna(axis=0, how='any', thresh=intVal, subset=['Col1', 'Col2'], inplace=False, ignore_index=False)\n",
        "    # axis --> 1/‘columns’ : Drop columns which contain missing value  (0/'index')\n",
        "    # how --> 'all': all values are NA, drop that row or column. 'any': one val is NA then drop that row/column)\n",
        "    # thresh --> number of NOT NULL values required to keep the row. Cannot be combined with how. # row containing non NaN values >= thresh are kept. how many non-null values you want to keep in a row= Thresh\n",
        "    # subset --> specifies where to look for NULL values\n",
        "    # ignore_index --> True : works like reset_index(inplace=True) [option not avaialble in the function]\n",
        "df.fillna(value, method, axis=0, inplace=False, limit=None, downcast) # fill with specified values, method\n",
        "    # value : value to replace the NULL values with | df.fillna(0) --> replace NaN with 0  | df.fillna(value={\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}) | df[\"Col\"].fillna(\"MissingVal\", inplace = True)\n",
        "    # method : method to use when replacing ('backfill', 'bfill', 'pad', 'ffill', None)\n",
        "    # limit : maximum number of NULL values to fill  | df.fillna(value=values, limit=1)  --> Only replace the first NaN element\n",
        "    # downcast : dictionary of values to fill for specific data types\n",
        "\n",
        "\n",
        "#==== related to column operations ======\n",
        "df[\"customer_country\"] == \"India\"                       # returns a dataframe where 'India' category is found under \"customer_country\" column\n",
        "df['ColName'].sum().round(2)                           # sum of the column contents and rounding off in case float number\n",
        "\n",
        "df[df['ColName'].str.startswith(\"Miss.\")]              # returns dataframe where category name starts with \"Miss.\" under specified column\n",
        "df.insert(intLocation,\"ColName\",\"int/str value\")       # adding new column to dataframe at Specified Location\n",
        "df['ColName'].sort_values()                            # returns sorted column with specified column\n",
        "df.sort_values('ColName')                              # returns sorted dataframe with specified column\n",
        "df.sort_values(by='ColName', ascending=\"True\",inplace=True)   # sorting the column contents (all df gets arranged as per sorted column contents)\n",
        "df.sort_values(['Age_in_Yrs', 'Age_in_Company','City'], ascending=[1,0,1])  # multiple columns; 0=desceding, 1=ascending\n",
        "df.sort_index(axis=1, ascending=False, , kind=<quicksort/mergesort/heapsort/stable>, na_position='last', ignore_index=False,) # Sort a Pandas DataFrame based on column names (sort by column names)\n",
        "                                                       # ignore_index = True: resulting axis will be labeled 0, 1, …, n - 1.\n",
        "\n",
        "df.nlargest(n=3, columns=['ColName'])                  # find n largest in the column\n",
        "df.nsmallest(n=3, columns=['ColName'])                 # find smallest in the column\n",
        "df['ColName'] = df['ColName'].str.upper() / lower() / title() / capitalize()     # change case/font of column contents\n",
        "df['ColName'].loc[0:2]                                 # get specific entry from specified column [index 0 entry : index 2 entry]\n",
        "\n",
        "df.set_index(keys='colName',inplace=True)       # set the specified column as an index, keys=['col1','col2'] multi-indexing\n",
        "read_csv(index_col='colName')                   # set the specified column as an index, index_col=['col1','col2'] multi-indexing\n",
        "df.loc['idx1Content','idx2Content']             # accessing the df contents using index contents, returns all col entries on the specified indexes\n",
        "df.loc['idx1Content','idx2Content'].'ColName'   # returns specified col entrie on the specified indexes\n",
        "df.loc[('idx1Content','idx2Content'),'ColName'] # another way of doing above\n",
        "\n",
        "#-------------------------------\n",
        "# Custom sort\n",
        "\"\"\"\n",
        "Case: if you have random but fixed number of categories and want to sort the dataframe using those random categories\n",
        "Example: A dataframe contains 'Name_of_student', 'Address', 'Pin_Code' and 'Marks'. Its easy to sort df using 'Marks',\n",
        "    but if 'Marks' column is not there in dataframe then... if we need to sort dataframe on the basis of 'Name_of_student' (not alphabetically)\n",
        "    in the custom sequence you want then follow the following code ....\n",
        "    Assume: df['Name_of_student'] contains names as sham,geeta,ram,seeta\n",
        "\"\"\"\n",
        "from pandas.api.types import CategoricalDtype  # install the library (to assign virtual number for sorting the category)\n",
        "cat_size_order = CategoricalDtype(['ram','sham','seeta','geeta'], ordered=True)  # ram<sham<seeta<geeta (our sequence) #list('Categories in custom order')\n",
        "df['Name_of_student'] = df['Name_of_student'].astype(cat_size_order)\n",
        "df = df.sort_values('Name_of_student')   # here is df with sorted rows with 1st_row:ram, 2nd_row:sham and so on....\n",
        "#-----------------------------------------------\n",
        "# bin continuous data into discrete intervals\n",
        "# divide continuous numbers/data into discrete intervals of categories\n",
        "import pandas as pd\n",
        "data = [1, 7, 5, 4, 6, 3, 8, 4, 2, 1]   # This is the list of numerical data that we want to categorize into bins.\n",
        "bins = [0, 2, 4, 6, 8]     # This defines the intervals for binning the data. It means: | Bin 1: 0 to less than 2 | Bin 2: 2 to less than 4 | Bin 3: 4 to less than 6 | Bin 4: 6 to less than 8\n",
        "bin_labels = ['very low', 'low', 'medium', 'high']   # labels assigned to each bin. There are four labels corresponding to the four bins defined by bins.\n",
        "binned_data = pd.cut(data, bins=bins, labels=bin_labels)\n",
        "print(binned_data)\n",
        "\n",
        "# ============================= group by ===========================================\n",
        "df.groupby(by='ColName')['Num_Column'].sum()               # get group categoriewise with sum of other numerical column\n",
        "df.groupby(by='ColName')['Num_Column'].sum().unstack()     # get above result in other easy format\n",
        "df.groupby(by='ColName')['Cat_Column'].value_counts()      # get group categoriewise with value counts of other categorical column (can use .unstack() here also)\n",
        "\n",
        "# group by specified columns and by considering other_columns. Specified operation is done on other_columns for grouping\n",
        "df_new = df.groupby(['col1', 'col2']).agg({'col3':'sum','col4':['max', 'sum']})  # 'mean', 'sum', 'max', 'min', 'std', 'count',  'median', 'var', 'quantile, 'nunique', 'first', 'last'\n",
        "df.groupby(['col1', 'col2'])[\"col3\", \"col4\"].apply(lambda x : x.astype(int).sum())\n",
        "#(ref: https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html)\n",
        "\n",
        "#--- get_group\n",
        "grp = df.groupby('Name')\n",
        "grp.get_group('Jai')\n",
        "# ================== mathematical operations on columns ===================================\n",
        "df[['Num_Col1', 'Num_Col2']].sum()  # returns = Num_Col1-82.29, Num_Col2-20.32 (returns colName and ans)\n",
        "df['Num_Col3'].mean()               # returns = 55.00 (returns only ans)\n",
        "df[['Num_Col3']].sum()              # returns = Num_Col3-36463  (returns colName and ans)\n",
        "\n",
        "# below agg() returns = specified maths operation of the specified columns in proper table format (better visuilization than above first three)\n",
        "df[['Num_Col1', 'Num_Col2','Num_Col3']].agg(['sum'/'mean'/'max'/'min']).round(2)\n",
        "df.agg({'Num_col1':['sum','mean'], 'Num_col2':['mean'], 'Cat_col1':['nunique']})  # for cat_col = value_counts, unique, nunique can be used\n",
        "#---\n",
        "get_max_cat = lambda x: x.value_counts(dropna=False).index[0]  # get cat_name which is occuring maximum times in df['Cat_col']\n",
        "df.agg({'Num_col': ['sum', 'mean'], 'Cat_col': [get_max_cat]}) # you can add a function (ex: get_max_cat) in agg()\n",
        "get_max_cat.__name__ = \"most_frequent\"   # it replaces 'lambda' name in the result table by 'most_frequent'\n",
        "#---\n",
        "# ================== missing value imputation by mean/mode/median ===========================\n",
        "df['ColName'].fillna(value=np.mean(df['ColName']),inplace=True)   # fill missing value in column by mean\n",
        "df['ColName'].fillna(value=df['ColName'].mean(), inplace=True)    # fill missing value in column by mean\n",
        "\n",
        "# ================== Rank ===================\n",
        "df['NewCol'] = df['ColName'].rank(ascending=True).astype('int')   # giving rank as per values in the column\n",
        "\n",
        "# ==================== finding the correlation among features =========\n",
        "df.corr()                         # correlation among numerical features of the dataset\n",
        "df1.corrwith(df2['Num_ColName'])  # correlation of df2['Num_ColName'] with each numerical col of dataframe df1\n",
        "# -------\n",
        "from dython.nominal import associations\n",
        "pd.set_option('max_colwidth', None)\n",
        "# df.select_dtypes(include='object', exclude='datetime64').columns     # if you want to view only categorical col from df\n",
        "complete_corr = associations(df)                                       # correlation among num or (default) categorical features of df\n",
        "df_unstruct = pd.DataFrame([complete_corr])\n",
        "df_complete_corr =list(df_unstruct['corr'])[0]\n",
        "df_complete_corr\n",
        "\n",
        "# =================== spelling correction maker ====================\n",
        "pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "spell = Speller()\n",
        "\n",
        "str1 = 'hospitalzasion'\n",
        "str2 = 'huspetalization'\n",
        "s1 = spell(str1)\n",
        "s2 = spell(str2)\n",
        "s1 == s2   # --> True\n",
        "\n",
        "# =================== replace 30K, 20L, 25C from 'Amount' column with 30000, 2000000, 250000000 ====================\n",
        "# Common function 'conversion(value)' for all three following methods:\n",
        "def conversion(value):        # Ex: value = $30K\n",
        "    multiplier = value[-1]    # K\n",
        "    amount = int(value[1:-1])  # 30\n",
        "    amount = amount*conversion_rate[multiplier]    # 30*1000\n",
        "    return(amount)             # 30000\n",
        "#--\n",
        "conversion_rate = {'K':1000, 'L':100000, 'C':10000000}  # just like conversion table, common to all following methodes\n",
        "# --- Method 1: using simple for loop\n",
        "converted_list = []           # list to store converted values\n",
        "for i in df['Amount']:\n",
        "    amount = conversion(i)               # call conversion function and convert amount one by one\n",
        "    converted_list.append(amount)        # keep on appending converted value one by one to list in each iteration\n",
        "df['Amount'] = converted_list      # assign all list contents (of converted amount) to the column\n",
        "# ------ Method 2: using list comprehension\n",
        "df['Amount'] = [conversion(i) for i in df['Amount']]       # type this first: conversion_rate = {'K':1000, 'L':100000, 'C':10000000}\n",
        "# ------ Method 3: using .apply() method\n",
        "df['Amount'] = df['Amount'].apply(conversion)  # type this first: conversion_rate = {'K':1000, 'L':100000, 'C':10000000}\n",
        "\n",
        "# ==============================================================================\n",
        "def conv(x):                    # function returns None,\n",
        "    print(x)                    # 'x' holds contents of a row/index received from a calling function (below line prints the same data)\n",
        "    print(x[['col1','col2']])   # prints the col1,col2 contents of the df (of which row/index number is received)\n",
        "    print(x['col1'])            # prints the col1 contents of the df (of which row/index number is received)\n",
        "    print()\n",
        "\n",
        "df=pd.DataFrame({'col1':[1,2,3], 'col2':[1,4,4]})\n",
        "df.apply(conv, axis=1)   # this function keeps on sending row (index) wise contents of df one by one to 'x' (till the end of df)\n",
        "# --------------------------------------------------------------\n",
        "def conv(x):      # function returns None\n",
        "    print(x)      # prints the values of df['col1'] one by one (till the end of df)\n",
        "\n",
        "df=pd.DataFrame({'col1':[1,2,3], 'col2':[1,4,4]})\n",
        "df['col1'].apply(conv)  # this function keeps on sending contents of df['col1'] row (index) wise one by one to 'x' (till the end of df)\n",
        "#----------------------------------------------------------------\n",
        "def conv(x):      # function returns None\n",
        "    print(x)      # prints all values of df['col1'] (till the end of df) & then print all values of df['col2'] (till the end of df)\n",
        "    print()\n",
        "\n",
        "df=pd.DataFrame({'col1':[1,2,3], 'col2':[1,4,4], 'col3':[6,5,4]})\n",
        "df[['col1','col2']].apply(conv)  # this function keeps on sending contents of df(col1 & col2) row (index) wise one by one to 'x' (till the end of df)\n",
        "# ----------------------------------------------------------------\n",
        "# put values in two new columns on the basis of exsisting column of the dataframe\n",
        "def conv(x):\n",
        "  if (some_condition):\n",
        "    # some task here..\n",
        "    return Val1Calculated_fromX, Val2Calculated_fromX\n",
        "  else:\n",
        "    return np.nan, np.nan\n",
        "\n",
        "df[['newCol1', 'newCol2']] = df['exsistingCol'].apply(conv).to_list()\n",
        "# NOTE: # exsistingCol != np.nan,0 (gives value error/col mismatch error) | exsistingCol == '','missingValue' (works properly)\n",
        "\n",
        "# ============================== filtering the data ================================\n",
        "# loc and iloc difference: df.loc[RowNo, ColName]       v/s         df.iloc[RowNo, ColNo]          # Access a group of rows and columns by label(s).\n",
        "# loc and iloc indexing  : df.loc[RowNo:RowNo, ColName:ColName] v/s df.iloc[RowNo:RowNo, ColNo:ColNo]   # Access a group of rows and columns by integer position(s).\n",
        "# at and iat work like loc and iloc (but it only Access a single value for a row/column label pair., does not support indexing or multiple value access)\n",
        "df.loc[2]               # retuns contents of 2nd row (same as: df.iloc[2])\n",
        "df.loc[5:7]             # returns 5th to 7th rows of the df (df.loc[5:7]  : returns 5th to 6th)\n",
        "df['ColName'].loc[5:7]  # returns 5th, 6th, 7th rows of df's specified column\n",
        "df.iloc[2,4]            # returns 'one' entry of df from 2nd row and 4th column\n",
        "df.iloc[[0,3],[1,4]]    # [[rowRange],[colRange]] returns entries from '0th and 3rd row' with '1st and 4th column'\n",
        "df.loc[0:5,['First_Name','Salary']]  # returns the specified range of rows of the given column in table format with given col data\n",
        "df.loc[0,'Weight_in_Kgs']     # returns value from 0th row and column named 'Weight_in_Kgs'\n",
        "df.loc[0][0]             # returns value from 0th row and 0th col\n",
        "df.loc[0]['ColName']     # returns value from 0th row and under the specified col name\n",
        "df.loc[df['CatColumn'].isin(['Category1','Category2',np.NaN])]  # returns whole df containing defined categories (Category1,2, Null)\n",
        "\n",
        "df.loc[df['StudName'] == 'Shaan', 'Fees'].sum()   # get the sum of fees from df where StudName==Shaan\n",
        "df.loc[Row/IndexNo, 'ColName'] = 10    # insert value at the specified location using loc\n",
        "\n",
        "df[df['Age'] > df['Weight']]         # returns whole df where Age val is gr than Weight val\n",
        "\n",
        "df[df[\"Age\"]<20][[\"Name\",'Age']]     # return Names of poeple whose Age is less than 20\n",
        "df[['Name','Age']][df['Age']<20]     # return Names of poeple whose Age is less than 20\n",
        "df.query('Age < 20')                 # returns whole dataset where df['Age']<20\n",
        "\n",
        "df[(df['Age']>50) & (df['Weight']>40)]     # returns the df where age>50 and weight>40\n",
        "df.query(\"Age > 50 & Weight > 40\")         # both does the same thing\n",
        "                                           # | No need to put column name in '' if col name is single | put category in ''\n",
        "                                           Ex: df.query(\"ColName1=='Category1' & ColName2=='Category2'\")\n",
        "\n",
        "df[df['Age'].between(20,28, inclusive='both')]    # return df where age is betwen 28 to 29\n",
        "\n",
        "df[df['Age']==40]                  # returns the dataframe where Age=40\n",
        "df[df['Age'].isin([40])]           # both does the same thing\n",
        "df[df['Age'].isin([74,28,19])]     # write condition in a list instead of writing seprately using OR\n",
        "\n",
        "df['Age'].idxmin()                  # returns the single index number where Age value is minimun\n",
        "df[df['Age']==min(df['Age'])].index # returns the all index numbers where Age value is minimun\n",
        "df[df['Age']==0].index              # retuns all index numbers where Age value is zero\n",
        "\n",
        "# filtering can be done by considering seprate conditions and later combining them\n",
        "c1 = df['Name'].str.contains('Mr')\n",
        "c2 = df['Age']>30\n",
        "c3 = df['Embarked']=='Q'\n",
        "df[(c1 & c2) | c3]\n",
        "\n",
        "#============================= duplicate values =====================================\n",
        "df.duplicated(subset=['Col1','Col2']], keep='first')  # first=first occurrence is set on False and all others on True./ last/ False=all duplicates are True.\n",
        "df.drop_duplicates(subset=['Col1', 'Col2'], keep='last',inplace = True, ignore_index = False)\n",
        "\n",
        "#================================ pin code / postal code / zip code / latitude longitude as per selected country ==================\n",
        "# pin_code to city_details\n",
        "pip install pgeocode\n",
        "import pgeocode\n",
        "nomi = pgeocode.Nominatim('in')  # in for india\n",
        "nomi.query_postal_code(\"362001\")   # returns place/address, state, county, lat-log details of inserted pin code\n",
        "nomi.query_postal_code(\"362001\").state_name  # returns name of the state\n",
        "# Ref : https://readthedocs.org/projects/pgeocode/downloads/pdf/latest/\n",
        "\n",
        "# city_name to Address-&-Pin Code\n",
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(user_agent=\"my-app\")\n",
        "location = geolocator.geocode('vile parle west', country_codes=\"IN\")\n",
        "location # returns whole address with pin code of given 'city'\n",
        "re.findall(r'\\d{6}', location[0]) # extracts 6 digit pin code from 'location' variable (import re) # retrns Null list if not found\n",
        "EX:  pinCode = int(''.join(re.findall(r'\\d{6}', geolocator.geocode(df.loc[i,'Hosp_City'], country_codes=\"IN\")[0])))\n",
        "#---------------------------------------------------------------\n",
        "\n",
        "import geocoder   # returns lat long from pin code (results are same as pgeocode)\n",
        "pincode = 15947\n",
        "print('City=', geocoder.arcgis(pincode).city)\n",
        "print('State=', geocoder.arcgis(pincode).state)\n",
        "print('Latitude=', geocoder.arcgis(pincode).lat)\n",
        "print('Longitude=', geocoder.arcgis(pincode).lng)\n",
        "# -- or --\n",
        "import geocoder    # considering Indian Pin codes only\n",
        "country = \"India\"\n",
        "pincode = \"360001\"\n",
        "query = f\"{pincode}, {country}\"\n",
        "location = geocoder.arcgis(query)\n",
        "latitude = location.lat\n",
        "longitude = location.lng\n",
        "\n",
        "#--------------Bubble map India---------------------\n",
        "import folium\n",
        "\n",
        "india_map = folium.Map(location=[20.5937, 78.9629], zoom_start=10)\n",
        "\n",
        "for index, row in df_ColNameSumYr[df_ColNameSumYr['latitude'].notnull()].iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['latitude'], row['longitude']],\n",
        "        radius=row['TotalClaimCountYr'] / 50000,\n",
        "        color='blue',\n",
        "        fill=True,\n",
        "        fill_color='blue',\n",
        "        fill_opacity=0.8,\n",
        "#         tooltip=row['PinCodeYr'] + ': ' + str(row['TotalClaimCountYr'])\n",
        "    ).add_to(india_map)\n",
        "\n",
        "india_map.save('bubble_map.html')\n",
        "india_map\n",
        "\n",
        "#==================== Create a drop down menu ===============================\n",
        "from ipywidgets import interact  # for dropdown menu\n",
        "def myfunction(x):\n",
        "    global cityName\n",
        "    cityName = x\n",
        "    return x\n",
        "interact(myfunction, x=list(df['City_Name'].unique()));\n",
        "print(cityName)\n",
        "#----------------------------------------------------------\n",
        "\n",
        "#========================= Read the characters from image (image to string) ======================\n",
        "import pytesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Shantanu.Kokate\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
        "pytesseract.image_to_string('test.png')    # Simple image to string (bypass the image conversions of pytesseract)\n",
        "pytesseract.image_to_string(Image.open('test.png'))  # Simple image to string\n",
        "print(pytesseract.image_to_string('images.txt')) # Batch processing with a single file containing the list of multiple image file paths\n",
        "# Timeout/terminate the tesseract job after a period of time\n",
        "try:\n",
        "    print(pytesseract.image_to_string('test.jpg', timeout=2)) # Timeout after 2 seconds\n",
        "    print(pytesseract.image_to_string('test.jpg', timeout=0.5)) # Timeout after half a second\n",
        "except RuntimeError as timeout_error:\n",
        "    # Tesseract processing is terminated\n",
        "    pass\n",
        "\n",
        "print(pytesseract.image_to_osd(Image.open('image.jpg')))  # Get information about orientation and script detection\n",
        "\n",
        "pdf = pytesseract.image_to_pdf_or_hocr('tempImage.jpg', extension='pdf')  # Get a searchable PDF (convert image into searchable pdf)\n",
        "with open('test.pdf', 'w+b') as f:\n",
        "    f.write(pdf) # pdf type is bytes by default\n",
        "\n",
        "#===================================================================================================\n",
        "\n",
        "https://www.myexcelonline.com/blog/vlookup-example-with-drop-down-list/\n",
        "update mdi HRMS pwd.. as .. user name : 19781, pwd: mdi@2022"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "################ combine() #################\n",
        "# returns the combined dataframe with min values from both the dfs\n",
        "df1.combine(df2, np.minimum)  # fill_value=-5, fill -5 at the place of NaN\n",
        "\n",
        "################ Concat() #################\n",
        "df1 = pd.DataFrame({'Student_ID': [1,3,5,2], 'Course_ID':[9,6,7,8]})\n",
        "df2 = pd.DataFrame({'Student_ID': [5,4,2,4,1], 'Course_ID':[10,8,3,2,7], 'Remarks':['50%', '20%', 'No', '30%','40%']})\n",
        "\n",
        "   Student_ID  Course_ID\n",
        "0           1          9\n",
        "1           3          6\n",
        "2           5          7\n",
        "3           2          8\n",
        "\n",
        "   Student_ID  Course_ID Remarks\n",
        "0           5         10     50%\n",
        "1           4          8     20%\n",
        "2           2          3      No\n",
        "3           4          2     30%\n",
        "4           1          7     40%\n",
        "\n",
        "# Col with same names in df will be concatinated one below another (concatination on the basis of column names, and not the contents)\n",
        "# In cancatinated df, index will remain as per old dataframes Ex:0,1,2,3,4 & 0,1,2,3\n",
        "    # if verify_integrity=True, throws error for such duplicate index number in cancatinated df\n",
        "    # Thus use, either ignore_index=True, works like df.reset_index() or For sepration of both concatinated df use, key=[])\n",
        "df_ConcatOneBelowAnother = pd.concat(objs=[df1,df2], ignore_index=True)\n",
        "   Student_ID  Course_ID Remarks\n",
        "0           1          9     NaN\n",
        "1           3          6     NaN\n",
        "2           5          7     NaN\n",
        "3           2          8     NaN\n",
        "4           5         10     50%\n",
        "5           4          8     20%\n",
        "6           2          3      No\n",
        "7           4          2     30%\n",
        "8           1          7     40%\n",
        "\n",
        "df_ConcatOneBelowAnotherMultiIndex = pd.concat(objs=[df1, df2], keys=['Table1','Table2'])  # 'Table1' & 'Table2' will be indexes with 1,2,3,4--index each\n",
        "# access row using : df.loc[('Table1', 1)] # access row element using : df.loc[('Table1', 1),'ColName']\n",
        "          Student_ID  Course_ID Remarks\n",
        "Table1 0           1          9     NaN\n",
        "       1           3          6     NaN\n",
        "       2           5          7     NaN\n",
        "       3           2          8     NaN\n",
        "Table2 0           5         10     50%\n",
        "       1           4          8     20%\n",
        "       2           2          3      No\n",
        "       3           4          2     30%\n",
        "       4           1          7     40%\n",
        "\n",
        "df_ConsiderCommonColumnToJoin = pd.concat(objs=[df1, df2], keys=['Table1', 'Table2'], join='inner')  # returns concatinated df with col common in both the dfs, default 'outer'\n",
        "          Student_ID  Course_ID\n",
        "Table1 0           1          9\n",
        "       1           3          6\n",
        "       2           5          7\n",
        "       3           2          8\n",
        "Table2 0           5         10\n",
        "       1           4          8\n",
        "       2           2          3\n",
        "       3           4          2\n",
        "       4           1          7\n",
        "\n",
        "df_ConcatOneAfterAnother = pd.concat(objs=[df1, df2], axis=1) # keys can be used for multi-indexing. join can be used for common row index concatination\n",
        "   Student_ID  Course_ID  Student_ID  Course_ID Remarks\n",
        "0         1.0        9.0           5         10     50%\n",
        "1         3.0        6.0           4          8     20%\n",
        "2         5.0        7.0           2          3      No\n",
        "3         2.0        8.0           4          2     30%\n",
        "4         NaN        NaN           1          7     40%\n",
        "\n",
        "####################### append() ###############################\n",
        "#appends two df one below other. (append operation is done only row-wise,not column-wise)\n",
        "df_append = df1.append(df2)   # returns same result as: pd.concat(objs=[df1, df2],join='outer')\n",
        "\n",
        "####################### merge() ###############################\n",
        "# if column names are different then it is not recommended to use concat(). Use merge(), join() methods.\n",
        "# same as sql join method, powerful than concat(), default: concat(join='outer'), merge(how='inner')\n",
        "\n",
        "pd.merge(df1,df2)  # it detects col with similar name in both dfs and merge two dfs on the basis of that common column (Default: how='inner')\n",
        "pd.merge(df1,df2, on='ColName') # if both dfs have more than 1 col with similar names, to avoid the confusion, specify col name on which merging is to be done\n",
        "pd.merge(df1,df2, on='ColName', suffixes=('_M1','_M2')) # after merging remaining columns with similar names will get suffix automatically, to avoid this use 'suffix=' of your choice\n",
        "pd.merge(df1,df2, left_on='C_ID', right_on='Cust_ID') # for col with different names: merginig is done on 'C_ID' col of df1 and 'Cust_ID' col of df2 (merging happens on matching contents of different col names of dfs)\n",
        "pd.merge(df1,df2, left_index=True, right_on='Cust_ID') # merginig is done on index of df1 and col of df2\n",
        "pd.merge(df2,df2, how='outer', indicator=True) # how = 'outer' : merge on the basis of common col and also return uncommon part of both the dfs\n",
        "                                               # how = 'left' (right/outer/inner/cross) : all row from left df is considered and only matched contents from right df will be considered (left df is imp and right not imp)\n",
        "                                               # indicator=True : tells from which df the row is taken (both/right_only/left_only)\n",
        "\n",
        "# NOTE : merge can also work like vlookup in excel. OR can be used for imputing values from other table's column on the basis of matching conditions of the tables\n",
        "    pd.merge(df1, df2, on='ColName', how='left')    # meaning: find df2 contents in df1\n",
        "  # the common contents under the specified (key=)'ColName' from both the dfs are considered and whole contents of df1 are returned while only matching contents of df2 are returned\n",
        "  # soemtimes above vlookup technic also works with : df[df['Age'].isin([40,43,56,32])]    # use for multiple entries\n",
        "###################### join() ###############################\n",
        "df1.join(df2) # joinig is done on the basis of index of both the columns # its like : pd.merge(df1,df2, left_index=True, right_on='Course_ID')"
      ],
      "metadata": {
        "id": "QEsyJpBeVvuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use it to decrease memory use for dataframe\n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "#------------------------\n",
        "reduce_mem_usage(df)    # use this function to reduce the memeory usage\n",
        "\n",
        "df.head()  # from here you can use ur dataframe"
      ],
      "metadata": {
        "id": "4OgcujqMHK7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAK5AIsvMlb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### read file in pandas"
      ],
      "metadata": {
        "id": "jOFfAHIbRd46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"data1.csv\")\n",
        "sheet_name=0  # str=\"Sheet1\", int=0(default),1,2--, list=[0, 1, \"Sheet5\"], or None, default 0\n",
        "sep='|',';',':',','(default)\n",
        "header=1               # row 1 becomes dataset header\n",
        "header=None            # CSV without column headers,  numbered columns starting from 0 are created\n",
        "names=['ColName1', 'ColName2', 'ColName3']          # rename the column headers by using the names parameter\n",
        "header=0, names=['ColName1', 'ColName2'] # header parameter overrides the old header names with new names\n",
        "header=None, prefix='column_'       # prefixes to the numbered column headers\n",
        "index_col='ColName'    # Making given column as index, =['ColName1', 'ColName2']      # Making multiple columns as index\n",
        "usecols= ['ColName1', 'ColName2']       # select only the necessary columns before loading the file in jupyter\n",
        "usecols=lambda x: len(x)>10)    # callable functions evaluate on column names to select that specific column where the function evaluates to True\n",
        "nrows=5        # Read the csv file with first 5 rows\n",
        "skiprows=1     # skip first row from the beginning of the file\n",
        "skiprows=lambda x: x%2!=0 # even indexed rows are returned\n",
        "skipfooter=1   # skip rows from the end of the file\n",
        "dtype={'ColName':np.int8}   # you can specify the data types of columns while reading the CSV file\n",
        "index_col='Date', parse_dates=True     #  convert a column to a datetime type column\n",
        "parse_dates=['Date']   # Pass desired column in parse_dates as list\n",
        "na_values=['missing']  # assign 'missing' string to NA values\n",
        "\n",
        "toInt = lambda x: int(x.replace(',', '')) if x!='missing' else -1\n",
        "converters={'ColName': toInt}   # convert the comma seperated values (i.e 98,12,341) of the column to integer value (9812341)\n",
        "\n",
        "https://www.machinelearningplus.com/pandas/pandas-read_csv-completed/\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "df = pd.read_excel('ExcelFileName.xlsx', sheet_name=None)  # Read all sheet names from the excel\n",
        "df.keys()        # returns the list of sheet names\n",
        "df1['sheetName'] # returns dataframe from the specified sheetName"
      ],
      "metadata": {
        "id": "KhP4w_WfRpE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------- pandas profiling report ----------------------\n",
        "# pip install ydata-profiling\n",
        "# import ydata_profiling\n",
        "# pip install ipywidgets\n",
        "import ydata_profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "Profile_Report = ProfileReport(df_selected, title=\"Profiling Report\")"
      ],
      "metadata": {
        "id": "Pe3kXNyORa4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------- save dataframe to excel --------------------\n",
        "pip install openpyxl\n",
        "import openpyxl\n",
        "import os\n",
        "\n",
        "df.to_excel('myexcel.xlsx', index=False)   # save df dataframe to excel\n",
        "\n",
        "excel_file_name = 'myexcel.xlsx'    # append tables to given sheet in already saved excel\n",
        "with pd.ExcelWriter(excel_file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
        "    df_to_save.to_excel(writer, sheet_name='MysheetName1', index=False, header=True)\n",
        "    df_to_save.to_excel(writer, sheet_name='MysheetName2', index=False, header=True)\n",
        "\n",
        "print('Please, check location to get generated excel file. :\\n',os.getcwd())"
      ],
      "metadata": {
        "id": "Ne81EuIHTxlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time Series"
      ],
      "metadata": {
        "id": "TSxMeOPSllJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Time Series ##########################\n",
        "https://www.alldatascience.com/time-series/forecasting-time-series-with-auto-arima/\n",
        "https://www.alldatascience.com/time-series/forecasting-time-series-with-arima/\n",
        "https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/\n",
        "https://techrando.com/2020/01/04/time-series-forecasting-using-a-seasonal-arima-model/\n",
        "https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
        "https://www.kaggle.com/code/poiupoiu/how-to-use-sarimax/notebook#SARIMAX-model\n",
        "https://analyticsindiamag.com/complete-guide-to-sarimax-in-python-for-time-series-modeling/\n",
        "https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.forecast.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.forecast\n",
        "https://www.kaggle.com/code/saurav9786/time-series-tutorial/notebook\n",
        "https://hackernoon.com/basic-understanding-of-arimasarima-vs-auto-arimasarima-using-covid-19-data-predicions-8o1v3x4t\n",
        "\n",
        "### Important functions requried in TS data analysis ###\n",
        "period = pd.Period('2019-1')     # returns, Period('2019-01', 'M')  --> specifies that it is monthly period\n",
        "period.asfreq('D', how='start')  # returns, Period('2019-01-01', 'D') --> conversion of above montly period to daily period\n",
        "period.to_timestamp().to_period('M')   # returns, Period('2019-01', 'M') --> convert period back to month\n",
        "period+3      # arithmatic opoerations can be done on period (it adds day/month etc in current day/month)\n",
        "\n",
        "pd.to_datetime(df['Date_Col'])   # converts date_string column to date_time column\n",
        "df['Date_Col'].astype('datetime64[ns]')\n",
        "  t = pd.Timestamp('2013-12-25 00:00:00') # time stamp to date conversion (pandas._libs.tslibs.timestamps.Timestamp) to datetime.date(yyyy, mm, dd)\n",
        "  t.date()\n",
        "  --> datetime.date(2013, 12, 25)\n",
        "\n",
        "df['Date_Col'] = df['Date_Col'] + pd.offsets.MonthEnd(0)  # set column dates to month end date\n",
        "\n",
        "pd.date_range(start='2022-9-1', periods=31, freq=\"D\")  # creates sequence of dates starts from given date considering period and frequency\n",
        "pd.date_range(end='2022-9-1', periods=31, freq=\"D\")   # creates sequence of dates from given period to end date date\n",
        "\n",
        "# changing time-series frequency\n",
        "    # for upsampling (less to more data), fill or handle the missing values. Ex: Q to M, M to W, W to D\n",
        "    # For downsampling (more to less data), aggregate the existing data. Ex: M to Q, W to M, D to W\n",
        "df.asfreq(freq='B', how='start') # frequency changes to Business days (sat-sun off)\n",
        "df.asfreq(freq='D'/'M'/'Q'/'W'/'Y', method='ffill/bfill')  # used in upsampling to fill up the data\n",
        "df['ColName'].interpolate()      # fill NA values in df (used in upsampling to fill up the data)\n",
        "\n",
        "df.resample('M')[\"price\"].sum()          # works only when date column is datetime and index\n",
        "df.resample('M'/'Q'/'W').mean().plot()   # For downsampling (more to less data), aggregate the existing data\n",
        "df.resample('W').agg(['mean', 'median', 'std'])\n",
        "df['data'].resample(rule='MS').mean()  # M,Q,W\n",
        "\n",
        "df.groupby('product_name')[\"price\"].resample(\"M\").sum()  # works only when date col = timedate+index col (below line gives the same o/p)\n",
        "df.groupby(['product_name', pd.Grouper(key='Date_Col', freq='W')])['price'].sum()  # workes only when date col is not index col (above line gives the same o/p)\n",
        "#(freq='A-DEC' means, an annual summary using December as the last month)\n",
        "\n",
        "print(df['2022-02'])      # returns only 2nd month dataframe (date col = datetime index)\n",
        "df.index.day/month/year   # returns day/month/year wise data from datetime index dataset (date col = datetime index)\n",
        "#--\n",
        "pv = pd.pivot_table(df, index=df.index.month, columns=df.index.year, values='Temp', aggfunc='sum')  # assuming datetime is index column\n",
        "pv.plot(subplots=True, legend=True, figsize=(15,15),grid=True)   # make yearwise columns from a TS data and plot yearwise line plot of entire df one below another\n",
        "pv.boxplot()/hist()/plot(kind='kde')\n",
        "#-_\n",
        "\n",
        "df['Date_Col'].dt.dayofyear        # returns day of the year. Ex: 05/01/2022 is 5th day of the year, returns 5\n",
        "df['Date_Col'].dt.dayofweek        # date falling in sunday = 0th day of week, mon=1, tue=2....sat=6\n",
        "df['Date_Col'].dt.is_leap_year     # returns the boolean value indicating if the date belongs to leap year\n",
        "\n",
        "df['Date_Col'].dt.hour /minute /second    # returns hour /minute /second of date in sequence as per stored in dataframe\n",
        "df['Date_Col'].dt.date /year /month /day  # returns all date/years/months/days in sequence as per stored in dataframe\n",
        "df['Date_Col'].dt.day_name()              # returns daydf_name of the given day_date\n",
        "df['Date_Col'].dt.month_name()            # returns month_names of the given day_date\n",
        "df['Date_Col'].dt.strftime(\"%A\")\n",
        "# %A -Full weekday name like MONDAY, TUESDAY etc\n",
        "# %a -Abbreviated weekday name (SUN,MON etc)\n",
        "# %w -Weekday as a decimal number like 1,2,3 etc (0:Sunday, 1:Monday, ----)\n",
        "# %Y -year, # %m -month, # %d -day, # %H -hours, # %M -minutes, # %S -seconds\n",
        "df['Date_Col'].dt.strftime('%d-%m-%Y')       # returns date in dd-mm-yyyy format\n",
        "\n",
        "df['Date_Col'].shift(2)  # shifts period/date into future by 2 days (2 NaN values at begining of df) - shift\n",
        "df['Date_Col'].shift(-2)  # shifts period/date into past by 2 days (2 NaN values at end of df) - lagged\n",
        "\n",
        "https://www.machinelearningplus.com/pandas/pandas-read_csv-completed/\n",
        "\n",
        "### working on TS data ####\n",
        "h0ttps://analyticsindiamag.com/complete-guide-to-sarimax-in-python-for-time-series-modeling/\n",
        "# ========================== Data Cleaning/ Preprocessing ==============================\n",
        "df.plot()                                 # plotting the data given\n",
        "df.dropna(inplace=True)                   # drop rows with NaN values\n",
        "df['Date'] = pd.to_datetime(df['Date'])   # converting date/day/month/year column to date time\n",
        "df['Date'] + pd.offsets.MonthEnd(0)       # sets date to end of the month instead of starting of the month ...\n",
        "                                              # ... Ex: it converts date from 2018-04-01 to 2018-04-30 and so on\n",
        "df.sort_values(by=\"Date\", inplace=True)   # sort the df by Date if it is not\n",
        "df.set_index('Date', inplace=True)        # set date/day/month/year column as an index\n",
        "# As we removed NaN values (with dates), lets find out missing dates in between using asfreq()\n",
        "# asfreq() works only if Date colm is an index column\n",
        "df.set_index(\"Date\", inplace=True).asfreq(\"D\")   # asfreq(\"D/W/M\"), you can set index here also instead of above row\n",
        "df.isnull().sum()                         # then check for missing/null Date values because of dropna() we used earliers, if it is showing some missing vlaues then use interpolate() as below\n",
        "df.interpolate(method ='linear', limit_direction ='forward')    # impute missing values corresponding new imputed dates using interpolate\n",
        "df.isnull().sum()                         # this should show 0\n",
        "df.reset_index(level=0)                   # you may reset the index here\n",
        "df.resample('W',on='Date').sum()          # daily/Date data can be resampled in week format\n",
        "\n",
        "# searching missing date from the Date column and imputing the missing date\n",
        "my_range = pd.date_range(start='2017-01-01', end='2017-09-01', freq='MS') # creating st+end date range as per dataset (MS: monthwise dates creation with starting date of each month)\n",
        "my_range.difference(df['Date_col'])  # Returns dates from my_range which are not present in df['Date_col'], ie returns missing dates from df['Date_col']\n",
        "                                    # not necessary to convert Date_column to date_range format\n",
        "\n",
        "df.set_index('Date',inplace=True)   # select Date column and set as index column\n",
        "df.reindex(my_range)          # and missing dates are imputed with Null Values\n",
        "\n",
        "# ========================== Visulizing the Data + Trend + Seasonal + Residual Graphs ==============================\n",
        "from pylab import rcParams\n",
        "import statsmodels.api as sm\n",
        "\n",
        "rcParams['figure.figsize'] = 18,15\n",
        "decomposition = sm.tsa.seasonal_decompose(df['VALUES'],model='additive',period=10)\n",
        "fig = decomposition.plot()\n",
        "plt.xticks(rotation= 90)\n",
        "plt.show()\n",
        "\n",
        "# ========================== Checking for data stationarity ==============================\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller   # import adfuller to use dicky-fuller test to test the data stationarity\n",
        "x = df['ValueColName'].values   # we need to pass array in the adfuller() to check stationarity of data column. convert Series into array using '.value'\n",
        "result = adfuller(x)            # returns values in tuple as, result = (adf, pvalue, usedlag, Critical_values)\n",
        "                    # printing each value of adfuller test using print() as follows\n",
        "                    # print(\"adf (The test statistic) = \",round(result[0],2))\n",
        "                    # print(\"pvalue =\", round(result[1],2))\n",
        "                    # print(\"used_lag (number of lags used) =\", result[2])\n",
        "                    # print(\"Num Of Observations Used For ADF Regression and Critical Values Calculation = \", result[3])\n",
        "                    # print(\"Critical values for the test statistic =\",result[4])\n",
        "                    # H0: data is non-stationary, H1: data is stationary\n",
        "\n",
        "if result[1]<= 0.05:              # result to check whether data is stationary or not\n",
        "    print(\"Reject H0, time series has no-unite root, the data is stationary\")\n",
        "else:\n",
        "    print(\"time series has unite root, the data is not stationary\")\n",
        "\n",
        "# ============================= if data is not stationary then make it stationary as follows ===================================\n",
        "#  make the data stationary using\n",
        "# 1. Differencing: values are shifted (by 1/2/3--12) and difference is taken (in 'newDiffcolumn'). NaN value rows must be dropped. Differencing is not used in fbProphet.\n",
        "# 2. Rolling mean: If RM=3, take a mean (not a difference) of first 3 values of main dataset and put that mean value in new dataset.\n",
        "# 3. Transformation: use log, sqrt, cuberoot, reciprocal transformation.\n",
        "\n",
        "# Differencing method as follows (NOTE: differencing etc can be done  especially on non-seasonal data)\n",
        "df['Diff_ValuesColumn'] = df['ValuesColumn'] - df['ValuesColumn'].shift(1)    # substract contents of column of it shifted by 1 and store the result in same dataframe with new column\n",
        "df.dropna(inplace=True)                   # drop rows with NaN values (NaN value was created by shift() function)\n",
        "df.replace(np.nan, 0, inplace=True)       # replace all occurances of 0 by NaN\n",
        "\n",
        "# ========================== if data is stationary then find Find p,d,q values (by using bar plot or line chart) ===================\n",
        "#---------- finding pdq values graphically:  Method-1 of 3 -------------------------\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# plot the bar chart\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "ax1 = fig.add_subplot(221)\n",
        "fig = sm.graphics.tsa.plot_acf(week_df2['diff_No_of_outstanding_cases'],lags=10,ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(week_df2['diff_No_of_outstanding_cases'], lags=10, ax=ax2)\n",
        "\n",
        "#---------- finding pdq values stastically: Method-2 fo 3 -------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "494NPg80N5GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZLqnQnXHA-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Integrating SQL server to Jupyter Notebook"
      ],
      "metadata": {
        "id": "Njq69ZpAGy-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run simple ms sql query with Python\n",
        "from sqlalchemy import create_engine         # import the library\n",
        "import pyodbc\n",
        "import pandas as pd\n",
        "\n",
        "Driver= 'SQL Server'                         # connection setup\n",
        "Server = r'MDINETPROJECTS\\Analytics'\n",
        "Database = 'Claims_SLA'\n",
        "UID = 'mdianalytics'\n",
        "PWD= 'mdianalytics@123'\n",
        "Database_Connection = f'mssql://{UID}:{PWD}@{Server}/{Database}?driver={Driver}'\n",
        "\n",
        "cnxn = pyodbc.connect(driver='{SQL Server}', host=Server, database=Database,\n",
        "                     user=UID, password=PWD)          # connection     # to close this connection use : connection.close()\n",
        "\n",
        "# importing the required columns to create desired dataframe\n",
        "df = pd.read_sql_query('''select * FROM [Claims_SLA].[dbo].[ML_SUM_LiablityAmt]\n",
        "where Min_SLA_Heading > '2020-12' and IC_name = 'The New India Assurance Company Limited' and Min_SLA_Heading IN ('2021-04','2021-05','2021-06','2022-04','2022-05','2022-06') ''', cnxn)\n",
        "cnxn.close()\n",
        "#==============================================================================\n",
        "#### if you have python variables from different DataBase to be included in the sql python query then  ####\n",
        "\n",
        "# suppose you want to extract data from different database in every query\n",
        "Driver= 'SQL Server'                         # connection setup\n",
        "Server = r'MDINETPROJECTS\\Analytics'\n",
        "# Database name is removed from here\n",
        "UID = 'mdianalytics'\n",
        "PWD= 'mdianalytics@123'\n",
        "Database_Connection = f'mssql://{UID}:{PWD}@{Server}?driver={Driver}'\n",
        "cnxn = pyodbc.connect(driver='{SQL Server}', host=Server, user=UID, password=PWD)          # connection     # to close this connection use : connection.close()\n",
        "\n",
        "DummyPol = '153400/00/24/00/00001114'  # python variables to be included in the python sql query\n",
        "LivePol = '153400/34/24/04/00000003'   # python variables to be included in the python sql query\n",
        "\n",
        "# including the variables in the parenthesis, and also mention Databse Name\n",
        "df = pd.read_sql_query(f'''\n",
        "select * from Masters.[dbo].Brok_Corp_Lives_Prem_ActivePol where Pol_no = '{DummyPol}'\n",
        "select * from Masters.[dbo].Brok_Corp_Lives_Prem_ActivePol where Pol_no = '{LivePol}'\n",
        "''' , cnxn)\n",
        "cnxn.close()\n",
        "\n",
        "#========================== If above doesnt work then use following code ==================================\n",
        "import pandas as pd\n",
        "import pyodbc\n",
        "\n",
        "Driver = 'SQL Server'\n",
        "Server = r'MDINETPROJECTS\\Analytics'\n",
        "UID = 'mdianalytics'\n",
        "PWD = 'mdianalytics@123'\n",
        "cnxn = pyodbc.connect(driver='{SQL Server}', server=Server, user=UID, password=PWD)  # Create the connection\n",
        "\n",
        "PolNo = '010100/00/22/0000000365'  # Define the variable\n",
        "query = '''  SELECT Pol_No, PolicyStartDate, PolicyEndDate FROM [Claims_SLA].[dbo].CL_CRS_DET2 WITH(NOLOCK) WHERE POL_NO = ? '''\n",
        "df = pd.read_sql_query(query, cnxn, params=(PolNo,))  # Execute the query with the parameter\n",
        "cnxn.close()\n",
        "df\n",
        "\n",
        "#==========================================================================================================\n",
        "### Get a variables, access variable from different tables of different databases  (WORKING CODE)\n",
        "from sqlalchemy import create_engine\n",
        "import pyodbc\n",
        "import pandas as pd\n",
        "#\n",
        "Driver= 'SQL Server'                         # connection setup\n",
        "Server = r'MDINETPROJECTS\\Analytics'\n",
        "UID = 'mdianalytics'\n",
        "PWD= 'mdianalytics@123'\n",
        "Database_Connection = f'mssql://{UID}:{PWD}@{Server}?driver={Driver}'\n",
        "cnxn = pyodbc.connect(driver='{SQL Server}', host=Server, user=UID, password=PWD)          # connection     # to close this connection use : connection.close()\n",
        "#\n",
        "DummyPol =  # variable\n",
        "LivePol =   # variable\n",
        "#\n",
        "db_table_dict = dict(DB1=['Table1','Table2'], DB2 = ['Table3','Table4'], DB3 = ['Table5','Table6'])    # Database name and its tables list\n",
        "TableName_ColName = {'Table1':'T1ColName', 'Table2':'T2ColName', 'Table3':'T3ColName', 'Table4':'T4ColName', 'Table5':'T5ColName', 'Table6':'T6ColName'}\n",
        " # table names with its col Name into which the variable needed to be checked\n",
        "\n",
        "for db_name, Tbl_name_list in db_table_dict.items():\n",
        "    for Tbl_name in Tbl_name_list:                                                                      # find the variable in the column of the table of given database\n",
        "        sql_query1 = f'''\n",
        "        select * from [{db_name}].[dbo].{Tbl_name} where {TableName_PolColName[Tbl_name]} = '{DummyPol}'\n",
        "        '''\n",
        "        df_dummy = pd.read_sql_query(sql_query1, cnxn)                                                     # function to read the variable from the table\n",
        "        #----------------\n",
        "        ql_query2 = f'''\n",
        "        select * from [{db_name}].[dbo].{Tbl_name} where {TableName_PolColName[Tbl_name]} = '{LivePol}'\n",
        "        '''\n",
        "        df_live = pd.read_sql_query(ql_query2, cnxn)\n",
        "\n",
        "cnxn.close()\n",
        "#===================================================================================================\n",
        "### if table is to be saved and not to be returned then use cursor (only internal operation on table)\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "import pyodbc\n",
        "import pandas as pd\n",
        "\n",
        "Driver= 'SQL Server'\n",
        "Server = r'MDINETPROJECTS\\Analytics'\n",
        "UID = 'mdianalytics'\n",
        "PWD= 'mdianalytics@123'\n",
        "Database_Connection = f'mssql://{UID}:{PWD}@{Server}?driver={Driver}'\n",
        "cnxn = pyodbc.connect(driver='{SQL Server}', host=Server, user=UID, password=PWD)\n",
        "\n",
        "cursor = cnxn.cursor()   # if table is to be saved and not to be returned then use cursor\n",
        "cursor.execute('''\n",
        "DROP TABLE IF EXISTS DBName1.dbo.MyTableName_temp\n",
        "SELECT <col1>, <col2> INTO <DBName1.dbo.MyTableName_temp>  FROM <[DBName].[dbo].[TableName]>\n",
        "WHERE <conditions>\n",
        "GROUP BY <col1>, <col2>\n",
        "''')\n",
        "cnxn.commit()\n",
        "\n",
        "df = pd.read_sql_query(''' select * from DBName1.dbo.MyTableName_temp ''', cnxn)  # read the saved table and use it in pandas\n",
        "cnxn.close()\n",
        "\n",
        "###################################################################\n"
      ],
      "metadata": {
        "id": "QIY65SOIGwIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crMlahRh0clv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install basemap\n",
        "# conda install -c anaconda basemap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "\n",
        "# Pin code coordinates (example: Mumbai)\n",
        "pin_code = '400001'\n",
        "latitude = 18.9387711\n",
        "longitude = 72.8353355\n",
        "\n",
        "# Create a basemap object for India\n",
        "m = Basemap(llcrnrlon=68, llcrnrlat=6, urcrnrlon=98, urcrnrlat=37, resolution='l')\n",
        "\n",
        "# Draw coastlines, country boundaries, and states\n",
        "m.drawcoastlines()\n",
        "m.drawcountries()\n",
        "m.drawstates()\n",
        "\n",
        "# Plot the pin code location on the map\n",
        "x, y = m(longitude, latitude)\n",
        "m.plot(x, y, 'ro', markersize=8, label=pin_code)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Show the map\n",
        "plt.title('Location from Pin Code')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TnpLG3alj7yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(lambda x, y: (x + y)(3, 8)\n",
        "(lambda x: x if x>10 else 'smaller than 10')(11)\n",
        "(lambda x: (x % 2 and 'odd' or 'even'))(3)\n",
        "\n",
        "#################################################\n",
        "\n",
        "def check_conditions(x):\n",
        "    if x > 10:\n",
        "        return x * 10\n",
        "    elif x < 5:\n",
        "        return x * 5\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "check_conditions(11)\n",
        "# --------------\n",
        "(lambda x: x * 10 if x > 10 else (x * 5 if x < 5 else x))(11)\n",
        "\n",
        "#################################################\n",
        "def doubling(x):\n",
        "    res = x * 2\n",
        "    return res\n",
        "#---------------------------\n",
        "a=10\n",
        "result = doubling(10)\n",
        "print(result)\n",
        "# --------------------------\n",
        "doubling = lambda x : x * 2\n",
        "result = doubling(20)\n",
        "result\n",
        "\n",
        "#################################################\n",
        "def addition(x,y):\n",
        "    return x+y\n",
        "\n",
        "result = addition(10, 20)\n",
        "result\n",
        "# -----------------------\n",
        "addition = lambda x,y : x + y\n",
        "result = addition(10, 20)\n",
        "result\n",
        "\n",
        "#################################################\n",
        "cube = lambda x : x*x*x\n",
        "cube(2)\n",
        "\n",
        "#################################################\n",
        "def func(a):\n",
        "    return lambda x : x*a\n",
        "\n",
        "double = func(2)  # a = 2\n",
        "double(10)          # x = 10\n",
        "\n",
        "#################################################\n",
        "def func(a):\n",
        "    return lambda x : x*a\n",
        "\n",
        "double = func(2)\n",
        "triple = func(3)\n",
        "\n",
        "print(double(10))\n",
        "print(triple(10))\n",
        "\n",
        "#################################################\n",
        "def doubling(x,y):\n",
        "    res1 = x * 2\n",
        "    res2 = y * 2\n",
        "    return res1, res2\n",
        "#----------------------\n",
        "a=10\n",
        "b=20\n",
        "result1, result2 = doubling(10,b)\n",
        "print(result1)\n",
        "print(result2)\n",
        "\n",
        "# A lambda function can take any number of arguments, but can only have one expression.\n",
        "lambda arguments : expression\n",
        "# therefore above function can not be implimented in lambda\n",
        "\n",
        "#################################################\n",
        "str1 = 'hello'\n",
        "resUpper = lambda x : x.upper()\n",
        "resUpper(str1)\n",
        "\n",
        "#################################################\n",
        "# lambda arguments : expression\n",
        "maxm = lambda x, y : x if x>y else y\n",
        "maxm(1,2)"
      ],
      "metadata": {
        "id": "-RfbQKV4p6t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook Markdown\n",
        "1) Horizontal lines : *** or --- or ___\n",
        "2) bold string : __bold string__ or **bold string**\n",
        "3) italic string : _italic string_ or *italic string*\n",
        "4) mathematical symbols : $ mathematical symbols $\n",
        "5) Monospace font : `Monospace font`\n",
        "6) Strikethrough : ~~Scratch Me.~~\n",
        "7) Indentation : > Text that will be indented when the Markdown is rendered.\n",
        "8) Task list :  - [x] Some task\n",
        "                - [ ] Some more task\n",
        "9) Colored text : <span style=\"color:red\">This is the text</span>\n",
        "                  color = blue|red|green|pink|yellow\n",
        "10) Font style : <span style=\"font-family:Comic Sans MS\">This is a text</span>\n",
        "11) Cell Background color : <code style=\"background:yellow;color:black\">CELL BACKGROUND COLOR</code>\n",
        "12) Highlight particular part of the text : Highlight <mark>this part</mark> of the text.\n",
        "13) Bullets :\n",
        "    - Bulleted item   or    - Bulleted item   or * Bulleted item\n",
        "    - Main bullet point\n",
        "      - Sub bullet point\n",
        "14) Blue box :\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Tip:</b> Use blue boxes (alert-info) for tips and notes.\n",
        "If it’s a note, you don’t have to include the word “Note”.\n",
        "</div>\n",
        "\n",
        "15) Yellow box :\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Example:</b> Use yellow boxes for warning or other.\n",
        "</div>\n",
        "\n",
        "16) Green box :\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Up to you:</b> Use green boxes for success.\n",
        "</div>\n",
        "\n",
        "17) Red box :\n",
        "<div class=\"alert alert-block alert-danger\">\n",
        "<b>Just don't:</b> In general, avoid the red boxes. These should only be\n",
        "used for actions that might cause data loss or another major issue.\n",
        "</div>"
      ],
      "metadata": {
        "id": "4hcZJnVGBrvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKTSYnlpyP7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqPlKR1ayP5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDp13E5oybwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to train subconMind give affermation in positive way\n",
        "\n",
        "write problems on papaer tear and burn it.\n",
        "write good things expected in life on paper.\n",
        "paramsukh, samadhan, aishwary, ..\n",
        "- affermation : in present contineus tense, present perfect tense. (one affermation 3 times, record and listen at night) - record the aims\n",
        "- vision borard : yellow page and red pen.\n",
        "- say thank you to water\n",
        "\n",
        "rei=pranUrja\n",
        "ki=gagruti\n",
        "Rediscovered by Dr Mecau ki (japan),\n",
        "5 tattva of rekie\n",
        "1. bring your life to 24 hrs\n",
        "2. control on anger\n",
        "3. aaj ke liye chintaon se dur rahunga\n",
        "4. aaj zalelya ashrivadachi ganati karavi\n",
        "5. har jivit prani ke prati me savedanshil rahunga"
      ],
      "metadata": {
        "id": "2c5DI07tyLLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://medium.com/@vipra_singh/llm-architectures-explained-nlp-fundamentals-part-1-de5bf75e553a\n"
      ],
      "metadata": {
        "id": "fbkkE-nsjkAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}